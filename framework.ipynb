{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2358b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from dataloader import *\n",
    "from  config import *\n",
    "from data_processor import preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cd4e618",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "Loading train dataset\n",
      "Loading images splits\n",
      "Images splits loaded\n",
      "Number of train images:  6000\n",
      "Number of test images:  1000\n",
      "Loading all captions\n",
      "All captions loaded\n",
      "Nuber of all captions:  8092\n",
      "Loading captions splits\n",
      "Captions splits loaded\n",
      "Number of train captions:  6000\n",
      "Number of test test:  1000\n",
      "Loading test dataset\n",
      "Loading images splits\n",
      "Images splits loaded\n",
      "Number of train images:  6000\n",
      "Number of test images:  1000\n",
      "Loading all captions\n",
      "All captions loaded\n",
      "Nuber of all captions:  8092\n",
      "Loading captions splits\n",
      "Captions splits loaded\n",
      "Number of train captions:  6000\n",
      "Number of test test:  1000\n"
     ]
    }
   ],
   "source": [
    "data = DataLoader(config_mixed_flickr8k_flickr8k_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16bd7aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train images:  6000\n",
      "Number of test images:  1000\n",
      "Number of train captions:  6000\n",
      "Number of test captions:  1000\n",
      "Descriptions cleaned.\n",
      "[['a', 'group', 'of', 'people', 'are', 'partying', 'at', 'a', 'masquerade', 'party'], ['a', 'lady', 'in', 'red', 'and', 'black', 'grins', 'at', 'the', 'camera', 'at', 'a', 'costume', 'party'], ['a', 'woman', 'in', 'a', 'domino', 'mask', 'and', 'severe', 'hair', 'is', 'at', 'a', 'party'], ['a', 'woman', 'in', 'a', 'red', 'dress', 'and', 'black', 'mask', 'is', 'on', 'a', 'crowded', 'dance', 'floor'], ['a', 'woman', 'wears', 'a', 'red', 'dress', 'and', 'a', 'black', 'mask', 'while', 'people', 'dance', 'behind', 'her']]\n",
      "Descriptions wraped into start and stop words.\n",
      "['START a child in a pink dress is climbing up a set of stairs in an entry way STOP', 'START a girl going into a wooden building STOP', 'START a little girl climbing into a wooden playhouse STOP', 'START a little girl climbing the stairs to her playhouse STOP', 'START a little girl in a pink dress going into a wooden cabin STOP']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-09 20:14:55.571085: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-09 20:14:55.571661: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 16. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed:\n",
      "100\n",
      "Processed:\n",
      "200\n",
      "Processed:\n",
      "300\n",
      "Processed:\n",
      "400\n",
      "Processed:\n",
      "500\n",
      "Processed:\n",
      "600\n",
      "Processed:\n",
      "700\n",
      "Processed:\n",
      "800\n",
      "Processed:\n",
      "900\n",
      "Processed:\n",
      "1000\n",
      "Test images encoded\n",
      "Time taken in seconds = 271.31609773635864\n",
      "Encoded test images saved under \n",
      "./mixed_flickr8k_8k_n/Pickle/encoded_test_images.pkl\n",
      "Processed:\n",
      "100\n",
      "Processed:\n",
      "200\n",
      "Processed:\n",
      "300\n",
      "Processed:\n",
      "400\n",
      "Processed:\n",
      "500\n",
      "Processed:\n",
      "600\n",
      "Processed:\n",
      "700\n",
      "Processed:\n",
      "800\n",
      "Processed:\n",
      "900\n",
      "Processed:\n",
      "1000\n",
      "Processed:\n",
      "1100\n",
      "Processed:\n",
      "1200\n",
      "Processed:\n",
      "1300\n",
      "Processed:\n",
      "1400\n",
      "Processed:\n",
      "1500\n",
      "Processed:\n",
      "1600\n",
      "Processed:\n",
      "1700\n",
      "Processed:\n",
      "1800\n",
      "Processed:\n",
      "1900\n",
      "Processed:\n",
      "2000\n",
      "Processed:\n",
      "2100\n",
      "Processed:\n",
      "2200\n",
      "Processed:\n",
      "2300\n",
      "Processed:\n",
      "2400\n",
      "Processed:\n",
      "2500\n",
      "Processed:\n",
      "2600\n",
      "Processed:\n",
      "2700\n",
      "Processed:\n",
      "2800\n",
      "Processed:\n",
      "2900\n",
      "Processed:\n",
      "3000\n",
      "Processed:\n",
      "3100\n",
      "Processed:\n",
      "3200\n",
      "Processed:\n",
      "3300\n",
      "Processed:\n",
      "3400\n",
      "Processed:\n",
      "3500\n",
      "Processed:\n",
      "3600\n",
      "Processed:\n",
      "3700\n",
      "Processed:\n",
      "3800\n",
      "Processed:\n",
      "3900\n",
      "Processed:\n",
      "4000\n",
      "Processed:\n",
      "4100\n",
      "Processed:\n",
      "4200\n",
      "Processed:\n",
      "4300\n",
      "Processed:\n",
      "4400\n",
      "Processed:\n",
      "4500\n",
      "Processed:\n",
      "4600\n",
      "Processed:\n",
      "4700\n",
      "Processed:\n",
      "4800\n",
      "Processed:\n",
      "4900\n",
      "Processed:\n",
      "5000\n",
      "Processed:\n",
      "5100\n",
      "Processed:\n",
      "5200\n",
      "Processed:\n",
      "5300\n",
      "Processed:\n",
      "5400\n",
      "Processed:\n",
      "5500\n",
      "Processed:\n",
      "5600\n",
      "Processed:\n",
      "5700\n",
      "Processed:\n",
      "5800\n",
      "Processed:\n",
      "5900\n",
      "Processed:\n",
      "6000\n",
      "Train images encoded\n",
      "Time taken in seconds = 1572.6800382137299\n",
      "Encoded train images saved under \n",
      "./mixed_flickr8k_8k_n/Pickle/encoded_train_images.pkl\n",
      "Number of training captions  30000\n",
      "Description Length: 37\n",
      "preprocessed words 7589 -> 1654\n",
      "Vocab size:  1655\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "data=preprocess_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1c1e2b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6a67206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector, \\\n",
    "    Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from keras.layers.merge import add\n",
    "from keras.models import Model\n",
    "from keras import Input, layers\n",
    "from keras import callbacks\n",
    "from eval_utils import calculate_results, prepare_for_evaluation\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from numpy import array\n",
    "import pickle\n",
    "from eval_utils import generate_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3d4fd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 37)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 37, 199)      329345      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2048)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 37, 199)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          524544      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          466944      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           dense_1[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1655)         425335      dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,811,960\n",
      "Trainable params: 1,811,960\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch, vocab_size):\n",
    "    \"\"\"\n",
    "    Data generator, that serves the data to the model during training\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    descriptions: str\n",
    "        Dictionary where key is image id and value is a list of preprocessed descriptions\n",
    "    photos\n",
    "        Dictionary with encoded images(vector of image features extracted by specified image feature extractor\n",
    "         fe. Inception), identified by image id\n",
    "    wordtoix\n",
    "        Dictionary with keys-words , values -id of word\n",
    "    max_length\n",
    "        Max number of words in caption on dataset\n",
    "    num_photos_per_batch: int\n",
    "    vocab_size: int\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n = 0\n",
    "    # loop for ever over images\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            n += 1\n",
    "            # retrieve the photo feature from the dictionary\n",
    "            photo = photos[key]\n",
    "            for desc in desc_list:\n",
    "                # encode the sentence by translating it to the number representation,\n",
    "                # with the dictionary of words created in in the previous stage\n",
    "                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
    "                # split one sequence into multiple X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # split into input and output pair\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    # encode output sequence\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    # store\n",
    "                    X1.append(photo)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            # yield the batch data\n",
    "            if n == num_photos_per_batch:\n",
    "                yield ([array(X1), array(X2)], array(y))\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n = 0\n",
    "\n",
    "class ModelImpl:\n",
    "    def __init__(self, data):\n",
    "        self.data=data\n",
    "        inputs1 = Input(shape=(2048,))\n",
    "        fe1 = Dropout(0.5)(inputs1)\n",
    "        fe2 = Dense(256, activation='relu')(fe1)\n",
    "        inputs2 = Input(shape=(self.data.max_length,))\n",
    "        se1 = Embedding(self.data.vocab_size, general[self.data.language][\"embedings_dim\"], mask_zero=True)(inputs2)\n",
    "        se2 = Dropout(0.5)(se1)\n",
    "        se3 = LSTM(256)(se2)\n",
    "        decoder1 = add([fe2, se3])\n",
    "        decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "        outputs = Dense(self.data.vocab_size, activation='softmax')(decoder2)\n",
    "        self.model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "        self.model.summary()\n",
    "        self.model.layers[2]\n",
    "\n",
    "        self.model.layers[2].set_weights([self.data.embedding_matrix])\n",
    "        self.model.layers[2].trainable = False\n",
    "\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer=self.optimizer())\n",
    "        self.setup()\n",
    "\n",
    "    def optimizer(self):\n",
    "        return Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    def setup(self):\n",
    "        # model.optimizer.lr = 0.001\n",
    "        self.epochs = 2\n",
    "        self.number_pics_per_bath = 100\n",
    "        self.steps = len(self.data.train_captions_wrapped) // self.number_pics_per_bath\n",
    "\n",
    "    def train(self):\n",
    "        model_weights_path=\"./\" + self.data.configuration[\"data_name\"] + self.data.configuration[\"model_save_dir\"]\n",
    "        if self.data.configuration[\"train_model\"]:\n",
    "            callback = callbacks.EarlyStopping(monitor='loss', min_delta=0.001, patience=3)\n",
    "            generator = data_generator(self.data.train_captions_wrapped,\n",
    "                                       self.data.encoded_images_train,\n",
    "                                       self.data.wordtoix,\n",
    "                                       self.data.max_length,\n",
    "                                       self.number_pics_per_bath,\n",
    "                                       self.data.vocab_size)\n",
    "            self.model.fit(generator, epochs=self.epochs,\n",
    "                           steps_per_epoch=self.steps,\n",
    "                           callbacks=[callback],\n",
    "                           verbose=1)\n",
    "            if self.data.configuration[\"save_model\"]:\n",
    "                writepath = model_weights_path+ \"/\"+'model' + '.h5'\n",
    "                self.model.save(writepath)\n",
    "                self.model.save_weights(model_weights_path\n",
    "                                        + self.data.configuration[\"model_save_path\"])\n",
    "        else:\n",
    "            self.model.load_weights(model_weights_path\n",
    "                                        +self.data.configuration[\"model_save_path\"])\n",
    "\n",
    "    def evaluate(self):\n",
    "        expected, results = prepare_for_evaluation(self.data.encoded_images_test, self.data, self.model)\n",
    "        out = calculate_results(expected, results, self.data.configuration)\n",
    "        print(out)\n",
    "\n",
    "model=ModelImpl(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55c9aad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "60/60 [==============================] - 497s 8s/step - loss: 5.0682\n",
      "Epoch 2/2\n",
      "60/60 [==============================] - 482s 8s/step - loss: 4.0723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateuszb/opt/anaconda3/envs/framework/lib/python3.7/site-packages/keras/engine/saving.py:165: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "  'TensorFlow optimizers do not '\n"
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4945ee14",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing for evaluation\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.04058218002319336,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 25,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855ada9c3fc34099b8c61aff59a8c465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed:\n",
      "0\n",
      "Processed:\n",
      "100\n",
      "Processed:\n",
      "200\n",
      "Processed:\n",
      "300\n",
      "Processed:\n",
      "400\n",
      "Processed:\n",
      "500\n",
      "Processed:\n",
      "600\n",
      "Processed:\n",
      "700\n",
      "Processed:\n",
      "800\n",
      "Processed:\n",
      "900\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 64178 tokens at 332638.84 tokens per second.\n",
      "PTBTokenizer tokenized 19766 tokens at 188432.80 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 18767, 'reflen': 12348, 'guess': [18767, 17767, 16767, 15767], 'correct': [5485, 1789, 395, 103]}\n",
      "ratio: 1.5198412698411468\n",
      "Bleu_1: 0.292\n",
      "Bleu_2: 0.172\n",
      "Bleu_3: 0.089\n",
      "Bleu_4: 0.046\n",
      "computing METEOR score...\n",
      "METEOR: 0.127\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.328\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.142\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/Users/mateuszb/PycharmProjects/test-framework/coco-caption/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\n",
      "WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Parsing reference captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.5 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [0.9 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [42.137 seconds]\n",
      "Parsing test captions\n",
      "Threads( StanfordCoreNLP ) [37.918 seconds]\n",
      "Warning: Nashorn engine is planned to be removed from a future JDK release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 1.513 min\n",
      "SPICE: 0.076\n",
      "computing WMD score...\n",
      "WMD: 0.383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Bleu_1': 0.2922683433686635, 'Bleu_2': 0.17154932282882232, 'Bleu_3': 0.08850610918173352, 'Bleu_4': 0.046131967365310785, 'METEOR': 0.12673418954682494, 'ROUGE_L': 0.32848613308537294, 'CIDEr': 0.14162510438388648, 'SPICE': 0.07603207168682966, 'WMD': 0.38274339838459465}\n"
     ]
    }
   ],
   "source": [
    "model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2e82d24",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Final results saved to final_results.csv\n"
     ]
    }
   ],
   "source": [
    "generate_report(general[\"results_directory\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfb6c69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17390a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c677e230",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "framework",
   "language": "python",
   "name": "framework"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}