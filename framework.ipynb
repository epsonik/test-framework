{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2358b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from dataloader import *\n",
    "from  config import *\n",
    "from data_processor import preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72ecdc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataloader2 import *\n",
    "# from  config2 import *\n",
    "# data = DataLoader(config_mixed_flickr8k_8k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cd4e618",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = DataLoader(config_mixed_flickr8k_flickr8k_n)\n",
    "# print(config_mixed_flickr8k_flickr8k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a1c1e2b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptions cleaned.\n",
      "[['a', 'group', 'of', 'people', 'are', 'partying', 'at', 'a', 'masquerade', 'party'], ['a', 'lady', 'in', 'red', 'and', 'black', 'grins', 'at', 'the', 'camera', 'at', 'a', 'costume', 'party'], ['a', 'woman', 'in', 'a', 'domino', 'mask', 'and', 'severe', 'hair', 'is', 'at', 'a', 'party'], ['a', 'woman', 'in', 'a', 'red', 'dress', 'and', 'black', 'mask', 'is', 'on', 'a', 'crowded', 'dance', 'floor'], ['a', 'woman', 'wears', 'a', 'red', 'dress', 'and', 'a', 'black', 'mask', 'while', 'people', 'dance', 'behind', 'her']]\n",
      "Descriptions wraped into start and stop words.\n",
      "['START a child in a pink dress is climbing up a set of stairs in an entry way STOP', 'START a girl going into a wooden building STOP', 'START a little girl climbing into a wooden playhouse STOP', 'START a little girl climbing the stairs to her playhouse STOP', 'START a little girl in a pink dress going into a wooden cabin STOP']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-07 01:04:07.863627: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-07 01:04:07.864114: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 16. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded train images loaded from: \n",
      "./mixed_flickr8k_8k_n/Pickle/encoded_train_images.pkl\n",
      "Encoded train images loaded from:  \n",
      "./mixed_flickr8k_8k_n/Pickle/encoded_test_images.pkl\n",
      "Number of training captions  30000\n",
      "Description Length: 37\n",
      "preprocessed words 7589 -> 1654\n",
      "Vocab size:  1655\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "data= preprocess_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6a67206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector, \\\n",
    "    Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from keras.layers.merge import add\n",
    "from keras.models import Model\n",
    "from keras import Input, layers\n",
    "from keras import callbacks\n",
    "from eval_utils import calculate_results, prepare_for_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3d4fd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 37)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 37, 199)      329345      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2048)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 37, 199)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          524544      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          466944      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           dense_1[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1655)         425335      dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,811,960\n",
      "Trainable params: 1,811,960\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class ModelImpl:\n",
    "    def __init__(self, data):\n",
    "        self.data=data\n",
    "        inputs1 = Input(shape=(2048,))\n",
    "        fe1 = Dropout(0.5)(inputs1)\n",
    "        fe2 = Dense(256, activation='relu')(fe1)\n",
    "        inputs2 = Input(shape=(self.data.max_length,))\n",
    "        se1 = Embedding(self.data.vocab_size, general[self.data.language][\"embedings_dim\"], mask_zero=True)(inputs2)\n",
    "        se2 = Dropout(0.5)(se1)\n",
    "        se3 = LSTM(256)(se2)\n",
    "        decoder1 = add([fe2, se3])\n",
    "        decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "        outputs = Dense(self.data.vocab_size, activation='softmax')(decoder2)\n",
    "        self.model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "        self.model.summary()\n",
    "        self.model.layers[2]\n",
    "\n",
    "        self.model.layers[2].set_weights([self.data.embedding_matrix])\n",
    "        self.model.layers[2].trainable = False\n",
    "\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer=self.optimizer())\n",
    "        self.setup()\n",
    "\n",
    "    def optimizer(self):\n",
    "        return Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    def setup(self):\n",
    "        # model.optimizer.lr = 0.001\n",
    "        self.epochs = 2\n",
    "        number_pics_per_bath = 100\n",
    "        self.steps = len(self.data.train_captions_wrapped) // number_pics_per_bath\n",
    "\n",
    "    def train(self):\n",
    "        if self.config[\"train_model\"]:\n",
    "            callback = callbacks.EarlyStopping(monitor='loss', min_delta=0.001, patience=3)\n",
    "            generator = data_generator(self.data.train_descriptions,\n",
    "                                       self.data.image_features_train,\n",
    "                                       self.data.wordtoix,\n",
    "                                       self.data.max_length,\n",
    "                                       self.number_pics_per_bath,\n",
    "                                       self.data.vocab_size)\n",
    "            self.model.fit(generator, epochs=self.epochs,\n",
    "                           steps_per_epoch=self.steps,\n",
    "                           callbacks=[callback],\n",
    "                           verbose=1)\n",
    "            if self.config[\"save_model\"]:\n",
    "                model_weights_path=\"./\" + self.config[\"data_name\"] + self.config[\"lstm_model_save_dir\"] \n",
    "                writepath = model_weights_path+ 'model' + '.h5'\n",
    "                self.model.save(writepath)\n",
    "                self.model.save_weights(model_weights_path\n",
    "                                        + self.config[\"lstm_model_save_path\"])\n",
    "        else:\n",
    "            self.model.load_weights(self.configuration[\"lstm_model_save_path\"])\n",
    "\n",
    "    def evaluate(self):\n",
    "        expected, results = prepare_for_evaluation(self.data.encoded_images_test, self.data, self.model)\n",
    "        out = calculate_results(expected, results, self.configuration)\n",
    "        print(out)\n",
    "\n",
    "\n",
    "model=ModelImpl(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55c9aad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02461385726928711,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 25,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7144c6afda934e7694df026eca9741ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'ModelImpl' object has no attribute 'config'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/6f/kl4hyt6j101gbd00913mjsr80000gn/T/ipykernel_45330/271906568.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mevaluate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/var/folders/6f/kl4hyt6j101gbd00913mjsr80000gn/T/ipykernel_45330/1314642712.py\u001B[0m in \u001B[0;36mevaluate\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     55\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mevaluate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     56\u001B[0m         \u001B[0mexpected\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresults\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mprepare_for_evaluation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencoded_images_test\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 57\u001B[0;31m         \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcalculate_results\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mexpected\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresults\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     58\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     59\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'ModelImpl' object has no attribute 'config'"
     ]
    }
   ],
   "source": [
    "model.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "framework",
   "language": "python",
   "name": "framework"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}